{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL) and Groq Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output:\n",
      " content=\"Groq is a query language developed by the company **Pinecone**, specifically designed for interacting with **vector databases**. \\n\\nHere's how it relates to LLMs:\\n\\n* **LLMs and Embeddings:** Large language models (LLMs) excel at understanding and generating human-like text. However, they don't directly understand structured data like databases. To bridge this gap, LLMs use **embeddings**: numerical representations of text that capture its meaning. These embeddings can be stored in vector databases.\\n\\n* **Vector Databases:** Vector databases are optimized for storing and querying these high-dimensional embeddings. They allow for efficient searching based on semantic similarity, rather than exact keyword matches.\\n\\n* **Groq as the Bridge:** Groq acts as a powerful query language for interacting with these vector databases. It allows you to:\\n    * **Search for similar embeddings:** Find documents or data points that are semantically related to a given query.\\n    * **Filter and refine results:** Narrow down your search based on specific criteria or properties.\\n    * **Combine queries:** Build complex searches by chaining together multiple Groq operations.\\n\\n**Example:**\\n\\nImagine you have an LLM-powered chatbot that needs to answer questions about a collection of product descriptions.\\n\\n1. The LLM processes the user's question and generates an embedding representing its meaning.\\n\\n2. This embedding is then used as a query in Groq to search the vector database containing product embeddings.\\n\\n3. The database returns the most semantically similar product descriptions, which the LLM can then use to provide a relevant answer.\\n\\n**Benefits of using Groq:**\\n\\n* **Intuitive and human-readable:** Groq syntax is designed to be easy to understand and use, even for non-technical users.\\n* **Powerful and flexible:** It offers a wide range of operators and functions for complex querying and data analysis.\\n* **Optimized for performance:** Groq queries are executed efficiently by vector databases, ensuring fast and responsive search results.\\n\\n\\n\\nIn essence, Groq provides a seamless way for LLMs to access and utilize the power of vector databases, unlocking new possibilities for semantic search, knowledge retrieval, and intelligent applications.\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 452, 'prompt_tokens': 19, 'total_tokens': 471, 'completion_time': 0.821818182, 'prompt_time': 0.0001058, 'queue_time': 0.01957327, 'total_time': 0.821923982}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-1190cd99-0ce9-4675-b285-fe8c1da8a96a-0' usage_metadata={'input_tokens': 19, 'output_tokens': 452, 'total_tokens': 471}\n",
      "\n",
      "Parser Output:\n",
      " Groq is a query language developed by the company **Pinecone**, specifically designed for interacting with **vector databases**. \n",
      "\n",
      "Here's how it relates to LLMs:\n",
      "\n",
      "* **LLMs and Embeddings:** Large language models (LLMs) excel at understanding and generating human-like text. However, they don't directly understand structured data like databases. To bridge this gap, LLMs use **embeddings**: numerical representations of text that capture its meaning. These embeddings can be stored in vector databases.\n",
      "\n",
      "* **Vector Databases:** Vector databases are optimized for storing and querying these high-dimensional embeddings. They allow for efficient searching based on semantic similarity, rather than exact keyword matches.\n",
      "\n",
      "* **Groq as the Bridge:** Groq acts as a powerful query language for interacting with these vector databases. It allows you to:\n",
      "    * **Search for similar embeddings:** Find documents or data points that are semantically related to a given query.\n",
      "    * **Filter and refine results:** Narrow down your search based on specific criteria or properties.\n",
      "    * **Combine queries:** Build complex searches by chaining together multiple Groq operations.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Imagine you have an LLM-powered chatbot that needs to answer questions about a collection of product descriptions.\n",
      "\n",
      "1. The LLM processes the user's question and generates an embedding representing its meaning.\n",
      "\n",
      "2. This embedding is then used as a query in Groq to search the vector database containing product embeddings.\n",
      "\n",
      "3. The database returns the most semantically similar product descriptions, which the LLM can then use to provide a relevant answer.\n",
      "\n",
      "**Benefits of using Groq:**\n",
      "\n",
      "* **Intuitive and human-readable:** Groq syntax is designed to be easy to understand and use, even for non-technical users.\n",
      "* **Powerful and flexible:** It offers a wide range of operators and functions for complex querying and data analysis.\n",
      "* **Optimized for performance:** Groq queries are executed efficiently by vector databases, ensuring fast and responsive search results.\n",
      "\n",
      "\n",
      "\n",
      "In essence, Groq provides a seamless way for LLMs to access and utilize the power of vector databases, unlocking new possibilities for semantic search, knowledge retrieval, and intelligent applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using model from Groq\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGroq(model='gemma2-9b-it')\n",
    "parser = StrOutputParser()\n",
    "response = llm.invoke('What is Groq in context of LLMs?')\n",
    "\n",
    "print('LLM Output:\\n', response)\n",
    "print('\\nParser Output:\\n', parser.invoke(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangServe is an open-source platform designed to streamline the process of deploying and serving large language models (LLMs).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Model Hosting:** It provides a framework for easily hosting and serving pre-trained LLMs, allowing developers to share and access models efficiently.\n",
      "* **API Endpoints:** LangServe exposes RESTful API endpoints for interacting with hosted models, making it easy to integrate LLMs into applications.\n",
      "* **Model Management:** It offers tools for managing multiple models, including versioning, deployment, and monitoring.\n",
      "* **Inference Optimization:** LangServe incorporates techniques to optimize inference performance, reducing latency and improving efficiency.\n",
      "* **Customizability:** Developers can customize the platform to suit their specific needs, such as adding new features or integrating with existing infrastructure.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Simplified Deployment:** Streamlines the process of deploying LLMs, reducing the complexity involved.\n",
      "* **Scalability:** Allows for scaling of model deployments to handle varying traffic loads.\n",
      "* **Accessibility:** Makes LLMs more accessible to developers by providing a user-friendly platform.\n",
      "* **Collaboration:** Facilitates collaboration among developers by enabling the sharing and reuse of models.\n",
      "* **Cost-Effectiveness:** Offers a cost-effective solution for deploying and serving LLMs compared to building custom infrastructure.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "* **Chatbots and Conversational AI:**\n",
      "\n",
      "Deploying chatbots powered by LLMs for customer service, education, or entertainment.\n",
      "* **Text Generation and Summarization:** Generating creative content, summarizing documents, or translating languages.\n",
      "* **Code Generation and Assistance:** Assisting developers with code completion, debugging, or documentation.\n",
      "* **Data Analysis and Insights:** Extracting insights from textual data, identifying patterns, or generating reports.\n",
      "\n",
      "**Overall, LangServe provides a robust and flexible platform for developers to leverage the power of LLMs in their applications.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_parser_chain = llm | parser\n",
    "print(llm_parser_chain.invoke('What is LangServe?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Message and System Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage('Translate this from English to French.'),\n",
    "    HumanMessage('Hello')\n",
    "]\n",
    "\n",
    "print(llm_parser_chain.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Translate the following into german', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you', additional_kwargs={}, response_metadata={})]\n",
      "[SystemMessage(content='Translate the following into german', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Using Prompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Translate the following into {language}'),\n",
    "    ('human', '{text}'),\n",
    "])\n",
    "\n",
    "response = prompt.invoke({'language':'german', 'text':'How are you'})\n",
    "\n",
    "print(response)\n",
    "print(response.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a few ways to say \"How are you?\" in German, depending on the level of formality:\n",
      "\n",
      "**Formal:**\n",
      "\n",
      "* **Wie geht es Ihnen?** (pronounced: vee gayt ess ee-nen)\n",
      "\n",
      "**Informal:**\n",
      "\n",
      "* **Wie geht es dir?** (pronounced: vee gayt ess deer) \n",
      "* **Wie geht's?** (pronounced: vee gayts) - This is a very casual way of asking.\n",
      "\n",
      "\n",
      "Choose the appropriate version based on the person you are speaking to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating chain with prompt\n",
    "prompt_llm_parser_chain = prompt | llm | parser\n",
    "\n",
    "print(prompt_llm_parser_chain.invoke({'language': 'German', 'text': 'How are you?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
