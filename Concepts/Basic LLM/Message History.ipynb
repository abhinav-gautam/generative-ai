{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Message history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Abhinav, that's great! Data Science is a fascinating and rewarding field.  \n",
      "\n",
      "What interests you most about data science? Do you have any experience with programming, statistics, or mathematics already? \n",
      "\n",
      "I can help you explore the path to becoming a data scientist. We can talk about:\n",
      "\n",
      "* **Essential skills:** What skills do you need to learn?\n",
      "* **Learning resources:** Where can you find good courses, books, and online tutorials?\n",
      "* **Projects:** What kind of projects can you build to showcase your skills?\n",
      "* **Career paths:** What are the different roles available in data science? \n",
      "\n",
      "Let me know what you'd like to discuss first!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "response = llm.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Hi, my name is Abhinav. I want to be a Data Scientist one day.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and no access to personal information about you. So I don't know your name or your goal.\\n\\nIf you'd like to tell me your name and goal, I'd be happy to listen! üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 20, 'total_tokens': 78, 'completion_time': 0.105454545, 'prompt_time': 7.549e-05, 'queue_time': 0.019630698, 'total_time': 0.105530035}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecbe5f6d-1507-4919-94b9-ba040f809307-0', usage_metadata={'input_tokens': 20, 'output_tokens': 58, 'total_tokens': 78})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No memory\n",
    "llm.invoke([HumanMessage(content=\"Do you know what is my name and my goal?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Yes, I do!  You introduced yourself as Abhinav, and you said you want to be a Data Scientist one day.  \\n\\nIs there anything specific you'd like to know or discuss about that goal right now?  Maybe we could brainstorm some steps you could take? üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 193, 'total_tokens': 257, 'completion_time': 0.116363636, 'prompt_time': 0.007193791, 'queue_time': 0.020257216, 'total_time': 0.123557427}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0cc6b551-9ddb-481d-b719-793c69a43196-0', usage_metadata={'input_tokens': 193, 'output_tokens': 64, 'total_tokens': 257})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With memory\n",
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Hi, my name is Abhinav. I want to be a Data Scientist one day.\"\n",
    "        ),\n",
    "        AIMessage(content=response.content),\n",
    "        HumanMessage(content=\"Do you know what is my name and my goal?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History\n",
    "\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create a store to store message histories based on session id\n",
    "store = {}\n",
    "\n",
    "\n",
    "# Function to retrieve the message history based on session id\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# LLM model that can work with message histories\n",
    "llm_with_message_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session config\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Abhinav, my name is Gemma.  \n",
      "\n",
      "It's nice to meet you! üòÑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is Abhinav, what is your name?\")], config=config\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Abhinav.  I remember! üòä  \n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")], config=config\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I have no memory of past conversations and no access to personal information about you, including your name. If you'd like to tell me your name, I'd be happy to use it!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing the session id\n",
    "config2 = {\"configurable\": {\"session_id\": \"chat2\"}}\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")], config=config2\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Prompt template with single variable Message Placeholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all the questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Abhinav! It's nice to meet you. \\n\\nI'm ready to answer your questions to the best of my ability. Just ask away! üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 32, 'total_tokens': 72, 'completion_time': 0.072727273, 'prompt_time': 0.00030438, 'queue_time': 0.020779396, 'total_time': 0.073031653}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-51092b7b-35fe-4485-be2e-f436420cf088-0', usage_metadata={'input_tokens': 32, 'output_tokens': 40, 'total_tokens': 72})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\": [\"My name is Abhinav\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Abhinav, I am Gemma!  It's nice to meet you. üòä \n",
      "\n",
      "What can I help you with today?  \n",
      "\n",
      "Your name is Abhinav.  üòä \n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Along with message history\n",
    "llm_with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hi my name is Abhinav. What is your name?\")],\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What is my name?\")],\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template with multiple variables\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the questions in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Namaste Abhinav! \\n\\n‡§Ü‡§™‡§ï‡•á ‡§®‡§æ‡§Æ ‡§∏‡•Å‡§®‡§ï‡§∞ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó‡§æ‡•§ \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§∏‡•á ‡§ï‡•Å‡§õ ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡§®‡•á ‡§ï‡§æ ‡§Æ‡•å‡§ï‡§æ ‡§Æ‡§ø‡§≤‡§®‡•á ‡§™‡§∞ ‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à‡•§ ‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡•á‡§Ç‡§ó‡•á? üòä\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 20, 'total_tokens': 73, 'completion_time': 0.096363636, 'prompt_time': 7.461e-05, 'queue_time': 0.020157298, 'total_time': 0.096438246}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-eee003d5-716b-422d-b518-9485343e524f-0', usage_metadata={'input_tokens': 20, 'output_tokens': 53, 'total_tokens': 73})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\": [\"My name is Abhinav\"], \"language\": \"hindi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§Ö‡§≠‡§ø‡§®‡§µ, ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§ú‡•á‡§®‡•§  \n",
      "\n",
      "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§Ö‡§≠‡§ø‡§®‡§µ ‡§π‡•à‡•§  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_with_message_history = RunnableWithMessageHistory(\n",
    "    chain, get_session_history, input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hi my name is Abhinav. What is your name?\")],\n",
    "        \"language\": \"hindi\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")], \"language\": \"hindi\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming message history\n",
    "\n",
    "If the conversation history is left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM model. Therefore it is important to trim the messages we are passing to the model.\n",
    "\n",
    "`trim_messages` is a helper which is used to reduce the number of messages we are sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always want to keep the system message and whether to allow partial messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§ï‡§ø‡§∏ ‡§§‡§∞‡§π ‡§ï‡§æ ‡§Ü‡§á‡§∏‡§ï‡•ç‡§∞‡•Ä‡§Æ ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à! ‡§Ü‡§™‡§ï‡§æ ‡§™‡§∏‡§Ç‡§¶‡•Ä‡§¶‡§æ ‡§Ü‡§á‡§∏‡§ï‡•ç‡§∞‡•Ä‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?  üç¶ üç®\n",
      "\n",
      "‡§Ü‡§™‡§®‡•á ‡§Æ‡•Å‡§ù‡§∏‡•á 2 + 2 ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•Ç‡§õ‡§æ ‡§•‡§æ‡•§  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chaining with trimmer\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer) | prompt | llm\n",
    ")\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"messages\": messages + [HumanMessage(content=\"What ice cream do I like?\")],\n",
    "    \"language\": \"Hindi\"\n",
    "})\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"messages\": messages + [HumanMessage(content=\"What math problem did I ask for?\")],\n",
    "    \"language\": \"Hindi\"\n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Abhinav,\n",
      "\n",
      "It's nice to meet you!  \n",
      "\n",
      "What can I do for you today? Do you have any questions I can answer or tasks I can help you with? üòä \n",
      "\n",
      "\n",
      "Your name is Abhinav.  \n",
      "\n",
      "I remember that you told me when you first introduced yourself! üòä  Is there anything else I can help you with?\n",
      "\n",
      "That's great! Vanilla ice cream is a classic for a reason.  \n",
      "\n",
      "Do you have a favorite way to enjoy it? With sprinkles, in a sundae, or just plain? üç¶\n",
      "\n",
      "You like vanilla ice cream!  üç¶  You told me so earlier. üòä  \n",
      "\n",
      "\n",
      "\n",
      "What's your favorite topping for it? \n",
      "\n",
      "That's awesome, Abhinav! Data Science is a really fascinating field. What do you enjoy most about it? \n",
      "\n",
      "Is there a particular area of data science that interests you the most?  ü§î \n",
      "\n",
      "\n",
      "Your favorite subject is Data Science!  You told me just now. üòä  \n",
      "\n",
      "Are you learning a lot in your Data Science classes?  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your name is Abhinav.  \n",
      "\n",
      "I remember!  Is there anything else I can help you with today? üòä \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wrapping with message history runnable\n",
    "chain = prompt | llm\n",
    "\n",
    "def get_trimmed_message_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "    \n",
    "llm_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_trimmed_message_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hi, my name is Abhinav\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")], \"language\": \"English\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"I like vanilla ice cream\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What ice cream do I like?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Data Science is my favorite subject\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What is my favorite subject?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = llm_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What is my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
