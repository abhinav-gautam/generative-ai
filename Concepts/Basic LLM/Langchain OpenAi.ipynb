{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-844807c2-cd0c-410a-836e-33de7ed7099b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is difference between ChatPromptTemplate and PromptTemplate?\n",
      "\n",
      "Formatted Prompt:\n",
      "Please answer the following query: What is difference between ChatPromptTemplate and PromptTemplate?\n",
      "\n",
      "Response:\n",
      "The difference between `ChatPromptTemplate` and `PromptTemplate` generally pertains to their intended use cases and the structure of the prompts they generate, especially in the context of working with language models or chatbots.\n",
      "\n",
      "1. **PromptTemplate**:\n",
      "   - This is typically used for generating text-based prompts that are meant for standard input-output interactions with a language model. It allows you to define a simple template for prompt creation where variables can be injected into a predefined format.\n",
      "   - Commonly utilized for tasks where the input and output are not necessarily conversational, but rather more straightforward, such as generating text based on a single prompt or question.\n",
      "\n",
      "2. **ChatPromptTemplate**:\n",
      "   - This is specifically designed for chat-based interactions. It accounts for the conversational nature of the interaction, allowing for multi-turn dialogue and the management of context across turns.\n",
      "   - It often includes features that help manage the back-and-forth nature of conversations, such as maintaining the context of previous messages, formatting prompts to reflect a dialogue format, and potentially including speaker roles (e.g., user vs. assistant).\n",
      "   - This is particularly useful when building chatbots or systems that require a more interactive and dynamic approach to conversation.\n",
      "\n",
      "In summary, while both templates serve to structure prompts for interacting with language models, `PromptTemplate` is more suited for straightforward, single-turn prompts, whereas `ChatPromptTemplate` is tailored for managing the complexities of conversational interactions.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# Define a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    template='Please answer the following query: {user_input}'\n",
    ")\n",
    "\n",
    "# Get the user input\n",
    "query = input('Enter your query:')\n",
    "\n",
    "# Format the prompt with user query\n",
    "formatted_prompt = prompt.format(user_input=query)\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(f'Query:\\n{query}\\n')\n",
    "print(f'Formatted Prompt:\\n{formatted_prompt}\\n')\n",
    "print(f'Response:\\n{response.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As of my last knowledge update in October 2023, LangSmith is a platform designed to assist developers and teams in building and managing applications that leverage large language models (LLMs). It provides tools and features that facilitate the integration of LLMs into various applications, helping users to streamline workflows, enhance productivity, and improve the overall development process.\\n\\nKey features of LangSmith may include:\\n\\n1. **Model Management**: Tools for managing different versions of language models and their configurations.\\n2. **Integration**: APIs and SDKs that make it easier to integrate LLMs into existing applications.\\n3. **Monitoring and Analytics**: Features to track the performance of language models in real-time and analyze their usage.\\n4. **Collaboration**: Support for teams to collaborate on projects involving LLMs, sharing insights and resources efficiently.\\n\\nLangSmith aims to simplify the complexities involved in working with large language models, making it accessible for developers of varying expertise levels. \\n\\nFor the most accurate and up-to-date information, I recommend visiting the official LangSmith website or checking recent publications about the platform.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 222, 'prompt_tokens': 33, 'total_tokens': 255, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None} id='run-7b7640db-7653-469a-b068-86e7f16dccc9-0' usage_metadata={'input_tokens': 33, 'output_tokens': 222, 'total_tokens': 255, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Chaining\n",
    "chain = prompt | llm\n",
    "\n",
    "# Querying chain\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last knowledge update in October 2023, LangSmith is a company that specializes in providing tools and solutions for building and optimizing applications that leverage large language models (LLMs). Their offerings typically include platforms that help developers integrate LLMs into their projects, enhancing functionality such as natural language processing, text generation, and conversational AI.\n",
      "\n",
      "LangSmith may focus on addressing challenges faced by developers when working with LLMs, such as improving model performance, managing data inputs and outputs, and ensuring that applications are scalable and efficient. They might also provide resources, documentation, and community support to facilitate the adoption of their tools.\n",
      "\n",
      "For the most up-to-date information regarding LangSmith, its products, and services, I recommend visiting their official website or checking recent news articles related to the company.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Querying chain\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
